{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # data processing, CSV file\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/CoinData/MyDrive/CoinData1'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999 #Outputs more colums for dataframes\n",
    "\n",
    "base = \"/CoinData/MyDrive/CoinData1\"\n",
    "datahistory = 60 #Amount of days for data history to use\n",
    "\n",
    "def coincount(data_drt): # counts amount of coin files within the given directory\n",
    "\n",
    "    coin__Count = 0\n",
    "    for cryptofile in os.listdir(data_drt):\n",
    "        if cryptofile.endswith(\".csv\"):\n",
    "            coin__Count += 1\n",
    "    return coin__Count\n",
    "\n",
    "\n",
    "def get_max_min_data_lengths(data_drt):\n",
    "    #It finds min and max lengths of the data coin files.\n",
    "    lengthmax = 0\n",
    "    lengthmin = float(\"inf\")  #Actiavtes lengthmin to be infinity\n",
    "    for cryptofile in os.listdir(data_drt):\n",
    "        if cryptofile.endswith(\".csv\"):\n",
    "            fD = pd.read_csv(data_drt + \"/\" + cryptofile, parse_dates=['Date'])\n",
    "            lengthdata = fD.shape[0]\n",
    "            lengthmax = max(lengthmax, lengthdata)\n",
    "            lengthmin = min(lengthmin, lengthdata)\n",
    "    return lengthmax,lengthmin\n",
    "\n",
    "\n",
    "def read_process_coin_data(data_drt, indexCoin):\n",
    "    # reads and processes a specific coin\n",
    "    cryptofile = os.listdir(data_drt)[indexCoin]\n",
    "    fD = pd.read_csv(data_drt + \"/\" + cryptofile, parse_dates=['Date'])\n",
    "    symbolCoin = cryptofile[5:-4]\n",
    "    lengthdata = fD.shape[0]\n",
    "    pricesclosing = fD['closed'].values\n",
    "    return symbolCoin, lengthdata, pricesclosing\n",
    "\n",
    "\n",
    "def read_data(data_drt, datahistory):\n",
    "    # reads and processes the cryptocurrency data\n",
    "    coinNum = coin__Count(data_drt)\n",
    "    maxlengthdata = get_max_min_data_lengths(data_drt)  #length_min discarded\n",
    "    pricesclosingdata = np.zeros((coinNum, maxlengthdata))\n",
    "    coinlengthsdata = np.zeros(coinNum, dtype=int)\n",
    "\n",
    "\n",
    "    for indexCoin in range(coinNum):\n",
    "        symbolCoin, lengthdata, pricesclosing = read_process_coin_data(data_drt, indexCoin)\n",
    "        print(indexCoin, symbolCoin, lengthdata)\n",
    "        priceslosingdata[indexCoin, 0:lengthdata] = pricesclosing\n",
    "        coinlengthsdata[indexCoin] = lengthdata\n",
    "\n",
    "\n",
    "def read_data (): # Reading and processing all data of coins\n",
    "\n",
    "    coinNum = 0\n",
    "    for name in os.listdir(base):\n",
    "        coinNum += 1 # counts the amount of files with coins\n",
    "\n",
    "    lengthmax, lengthmin = 0, 1000000 # intiating a big value\n",
    "    for name in os.listdir(base):\n",
    "        fD = pd.read_csv(base + \"/\" + name, parse_dates=['Date'])\n",
    "        thelength = fD.shape[0]\n",
    "        if lengthmax < thelength:\n",
    "            lengthmax = thelength\n",
    "        if lengthmin > thelength:\n",
    "            lengthmin = thelength\n",
    "\n",
    "\n",
    "    info = np.zeros ((coinNum, lengthmax)) #Activates arrays to keep coins data\n",
    "    Slength = np.zeros(coinNum, dtype = int)\n",
    "    i = 0\n",
    "    for name in os.listdir(base):\n",
    "        coinSymb = name[5:-4] #Extracting the symbol of the coin from the file name.\n",
    "        fD = pd.read_csv(base + \"/\" + name, parse_dates=['Date'])\n",
    "        thelength = fD.shape[0]\n",
    "\n",
    "        #Storing coin info.\n",
    "        Slength[i] = thelength\n",
    "        print (i, coinSymb, thelength)\n",
    "\n",
    "        info[i, 0:thelength] = fD['Close'].values\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    return coinNum, Slength, info\n",
    "\n",
    "coinNum, Slength, info = read_data ()\n",
    "print (\"Coin amount: \", coinNum, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scale(info, Slength):\n",
    "\n",
    "    coinNum = info.shape[0] #More cleaner name for the variable.\n",
    "    shift_info = np.zeros(coinNum)\n",
    "    factor_scale = np.zeros(coinNum)\n",
    "    #^^^ shift_info and factor_scale have descriptive name.\n",
    "\n",
    "    for i in range (coinNum):\n",
    "        valMax = info[i,:Slength[i]].max()\n",
    "        valMin = info[i, :Slength[i]].min()\n",
    "        #^^^ Clearing calculation ^^^\n",
    "        shift_info[i] = valMin #This stores the info of shift value of the info.\n",
    "        factor_scale[i] = valMax - valMin #Calculating the factor scale.\n",
    "\n",
    "        if factor_scale[i] == 0:\n",
    "            raise ValueError(\"Division by zero encountered during scaling.\")\n",
    "\n",
    "        info[i,0:Slength[i]] = (info[i,0:Slength[i]]-shift_info[i])/factor_scale[i]\n",
    "    return (shift_info, factor_scale) #Scaling the info and scale factor\n",
    "\n",
    "shift_info, factor_scale = data_scale (info, Slength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence(info, Slength, begin, finish, datahistory):\n",
    "    assert len(info) == len(Slength), \"Unmatched data length.\"\n",
    "\n",
    "    # Initialize blank lists to keep targets and sequences\n",
    "    A = []\n",
    "    B = []\n",
    "\n",
    "    # Iterate over the specified range of coins\n",
    "    for j in range(begin, finish):\n",
    "        for i in range(datahistory, Slength[j]):\n",
    "            # Append input sequence and its compatible value target\n",
    "            A.append(info[j, i - datahistory:i])\n",
    "            B.append(info[j, i])\n",
    "\n",
    "    X = np.array(A)[:, :, np.newaxis] # Converting lists to numpy arrays and reshape for LSTM input\n",
    "    y = np.array(B)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# number of days for data history to use ie 15 days\n",
    "datahistory = 15\n",
    "\n",
    "# Creating the validation and training, and test sequences\n",
    "X_val, Y_val = sequence(info, Slength, 18, 22, datahistory)\n",
    "print(\"Approximately\", Y_val.shape[0], \"sequences for validation.\")\n",
    "\n",
    "X_test, Y_test = sequence(info, Slength, 22, 23, datahistory)\n",
    "print(\"Approximately\", Y_test.shape[0], \"sequences for testing.\")\n",
    "\n",
    "X_train, Y_train = sequence(info, Slength, 0, 18, datahistory)\n",
    "print(\"Approximately\", Y_train.shape[0], \"sequences for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def lstm_model(datahistory):\n",
    "\n",
    "    model_lstm = tf.keras.Sequential()\n",
    "\n",
    "    # 1st LSTM layer with 130 units, returning sequences for a larger model\n",
    "    model_lstm.add(tf.keras.layers.LSTM(130, return_sequences=True, input_shape=(datahistory, 1)))\n",
    "\n",
    "    model_lstm.add(tf.keras.layers.LSTM(70, return_sequences=False)) # The second LSTM layer with 70 units, not returning sequences.\n",
    "\n",
    "    model_lstm.add(tf.keras.layers.Dense(30)) # A Dense layer with 30 units for intermediate processing.\n",
    "\n",
    "    model_lstm.add(tf.keras.layers.Dense(1)) # output Dense layer with 1 unit for prediction.\n",
    "\n",
    "    model_lstm.compile(optimizer='adam', loss='mean_squared_error') # compiling the model using the Adam optimizer and mean squared error loss\n",
    "\n",
    "    model_lstm.summary() # Summary of the model for an overview\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "\n",
    "datahistory = 15  # number of past observations that we have used\n",
    "model_lstm = lstm_model(datahistory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training process. It stores the training history for examination, indication of a validation set, adjusting batch size for optimization,\n",
    "#And epochs increased for the machine to have better learning.\n",
    "training = model_lstm.fit(X_train, Y_train, validation_data = (X_val, Y_val), batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def stats_reveal (training):\n",
    "    #Plotting the validation and training curves of loss.\n",
    "    plt.plot(training.history['loss'])\n",
    "    plt.plot(training.history['val_loss'])\n",
    "\n",
    "\n",
    "    #Setting the graph labels and tilte.\n",
    "    plt.title(\"Model\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss Prediction\")\n",
    "    plt.legend([\"Training Loss\", \"Validation Loss\"])\n",
    "\n",
    "    #Outputs the graph.\n",
    "    plt.show()\n",
    "\n",
    "#Summons the function to get a visualization on the curves loss.\n",
    "stats_reveal (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate the model's predictions on the test dataset\n",
    "prognosis = model_lstm.predict(X_test)\n",
    "\n",
    "plt.title(\"Model's prediction vs real prices\")\n",
    "\n",
    "# calculating the (RMSE) to determine accuracy of prediction\n",
    "\n",
    "rmse = np.sqrt(np.mean(((prognosis - Y_test) ** 2)))\n",
    "print(\"RSME: \", rmse)\n",
    "\n",
    "#Plot : prediction data.\n",
    "plt.plot(prognosis*factor_scale[20] + shift_info[20])\n",
    "\n",
    "#Plots : real data.\n",
    "plt.plot(Y_test*factor_scale[20] + shift_info[20])\n",
    "\n",
    "#Output graph\n",
    "plt.legend([\"Predictions\",\"Real data\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
